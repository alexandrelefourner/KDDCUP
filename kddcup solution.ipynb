{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple demonstration of Artificial Neural Network applied to KDDCUP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "The dataset can be found at <a href=\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\">this adress</a>.\n",
    "\n",
    "The objective is to determine wether the traffic is normal or not(last column of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#We start the import\n",
    "\n",
    "import pandas as pd #Used to read the dataset file\n",
    "import numpy as np #Used for quick manipulation of the columns\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm #Will display a nice animation while loading.\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import urllib #Used to data architecture.\n",
    "\n",
    "from tensorflow import keras #Used for our neural network.\n",
    "from tensorflow.keras import layers #Will define what's inside the neural network.\n",
    "\n",
    "from sklearn.model_selection import train_test_split #Will be used to split the testing and training data.\n",
    "from sklearn.metrics import confusion_matrix #Used to see the efficiency of the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2   3    4     5   6   7   8   9   ...  32   33   34    35   36  \\\n",
       "0   0  tcp  http  SF  181  5450   0   0   0   0  ...   9  1.0  0.0  0.11  0.0   \n",
       "1   0  tcp  http  SF  239   486   0   0   0   0  ...  19  1.0  0.0  0.05  0.0   \n",
       "2   0  tcp  http  SF  235  1337   0   0   0   0  ...  29  1.0  0.0  0.03  0.0   \n",
       "3   0  tcp  http  SF  219  1337   0   0   0   0  ...  39  1.0  0.0  0.03  0.0   \n",
       "4   0  tcp  http  SF  217  2032   0   0   0   0  ...  49  1.0  0.0  0.02  0.0   \n",
       "\n",
       "    37   38   39   40       41  \n",
       "0  0.0  0.0  0.0  0.0  normal.  \n",
       "1  0.0  0.0  0.0  0.0  normal.  \n",
       "2  0.0  0.0  0.0  0.0  normal.  \n",
       "3  0.0  0.0  0.0  0.0  normal.  \n",
       "4  0.0  0.0  0.0  0.0  normal.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We start by loading the dataset we will work on. I work here on the 10% dataset.\n",
    "df = pd.read_csv(\"kddcup.data_10_percent_corrected\", header = None)\n",
    "df.head() #We display the 5 first row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "### Extracting labels and formating them\n",
    "Neural networks do not understand inputs like \"Normal, \"http\", \"udp\". We need to work with numerical values. The purpose of the data processing is to reshape the data to have a proper \"communication\" with the neural network.\n",
    "\n",
    "We will use a feedforward neural network, which takes 1 neuron as output, firing 1 if normal, 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different objects confirms\n"
     ]
    }
   ],
   "source": [
    "#We split the output of the neuron (the answer we look for, that we will call a \"label\")\n",
    "Y = df[41] #our labels.\n",
    "\n",
    "#We next take the inputs. The information we will pass through the network to predict if the traffic is normal or not.\n",
    "X = df.drop(columns=[41])\n",
    "\n",
    "#Next, we reshape the structure, from a String \"normal.\" to 1... or 0 if not \"normal.\"\n",
    "Y = np.where(Y.get_values() == \"normal.\", 1, 0)\n",
    "\n",
    "#Next, we control that we have different types of outputs in dataset (we check that we don't have only \"normal.\" elements..\n",
    "if np.sum(Y) != Y.shape[0]:\n",
    "    print(\"Different objects confirms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "If you look at http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names which presents the list of the features, you will see there are multiple types of features.\n",
    "- Continuous: Values that are numeric, but not necessary between 0 and 1 (what we will prefer to train the neural network)\n",
    "- Symbolic: A sign represents something different. There might be more than 2 elements and we cannot use a simple neuron to fire values between 0 and 1 to represent this concept. instead, we will use a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\">one hot encoding with get_dummies() function from pandas</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leximus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a1aa3109fe48e8870cc0971936a0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing columns:\n",
      "[1, 2, 3, 6, 11, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "#First : we collect the list of continuous features.\n",
    "bases = urllib.request.urlopen(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\").read(10000).decode(\"utf-8\") \n",
    "bases = bases.split(\"\\n\")[1:-1]\n",
    "cols_continuous = []\n",
    "\n",
    "for bi, bcontent in enumerate(bases):\n",
    "    if \"continuous\" in bcontent:\n",
    "        cols_continuous.append(bi)\n",
    "\n",
    "#We now have the list. We are about to correct the input to format them to the good shape.\n",
    "\n",
    "#Will list the columns to remove\n",
    "#(containing symbolic elements, because they will be replaced by new columns of a one_hotèshape)\n",
    "col_to_rem = [] \n",
    "\n",
    "#Since we reshape the dataframe, we create a copy of the column list to not iterate through our newly created columns.\n",
    "copy_columns = X.columns\n",
    "\n",
    "#We look the columns, one by one.\n",
    "for col in tqdm(copy_columns):\n",
    "    if col in cols_continuous: #This is a continuous data. We reshape it to the range [0;1]\n",
    "        X[col] = (X[col]-X[col].min())/(X[col].max() - X[col].min()) #Way to reshape it through \"Min Max Scaling\"\n",
    "    else:#Symbolic data.\n",
    "        col_to_rem.append(col) #We will remove this column\n",
    "        X = pd.concat([X, pd.get_dummies(X[col])], axis=1); #And we add the one hot encoded version.\n",
    "\n",
    "print(\"Removing columns:\")\n",
    "print(col_to_rem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnecessary data and NaN\n",
    "\n",
    "We now have the columsn to remove in the col_to_rem variable.\n",
    "However, our dataset contains NaN values. This value represent missing information (incomplete informations).\n",
    "We replace all NaN value by \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 118)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.drop(columns=col_to_rem) #Cleaning columns\n",
    "X = X.fillna(0) #Cleaning missing values.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 494021 samples of 118 dimensions.\n",
    "1 dimension equals to 1 information we have in each sample.\n",
    "\n",
    "We now our neural network will have 118 dimension as input.\n",
    "We now it will have 1 dimension as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Network\n",
    "\n",
    "I chose to use a very smal neural network of 3 layers : The input, an output layer, and a layer in between that we will call \"hidden layer\".\n",
    "\n",
    "<img src=\"img/neural_network.png\">\n",
    "\n",
    "I chose to put 16 neurons in the hidden layer. \n",
    "\n",
    "We can add more layers and more neurons... Or even less. The more layers you add, the more you increase the complexity which can be captured by the network. This attitude however tend to require more computation time and more data. It is also more prone to overfitting.\n",
    "\n",
    "The smaller the network, the better.<br><br><br>\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=woa34ugDSwY\"><img src=\"img/meme1.jpg\">\n",
    "    <center>https://www.youtube.com/watch?v=woa34ugDSwY</center></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 18:58:06.085824 19500 deprecation.py:506] From c:\\users\\leximus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_ann\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 118)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1904      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,985\n",
      "Trainable params: 1,953\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We define our entrance.    \n",
    "inputs = keras.Input(shape=(118,))\n",
    "\n",
    "#We connect 16 neurons to this entrance. The data will flow to this neurons.\n",
    "x = layers.Dense(16, activation=\"relu\")(inputs)\n",
    "\n",
    "#We add a BatchNormalization.\n",
    "#This will help the value to stay between 0-1 inside the neural network, proving a better stability.\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "#We next create an output layer which will give the prediction of the neural network.\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "#The model is now ready to be created.\n",
    "model = keras.Model(inputs=inputs, outputs=out, name=\"simple_ann\")\n",
    "\n",
    "#We display the structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains around 2000 parameters to train, which is a small number. This will help to train faster the network. If the model does not capture enough accuracy, we will try to inrease the number of neurons/layers.\n",
    "\n",
    "(do not hesitate to have a look with the column \"OutputShape\" and the diagram of the network above!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 18:58:06.181569 19500 deprecation.py:323] From c:\\users\\leximus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#We now \"build\" the network, defining how we want to train it.\n",
    "\n",
    "#Binary crossentropy defines the loss. It will tell how the network make mistake (and thus, how to converge to the solution.)\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.SGD(), #The strategy used to reduce the error. Here, SGD.\n",
    "    metrics=[\"accuracy\"]) #We display the accuracy of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset : A system to detect overfitting\n",
    "When we train the network, we show samples `S`.\n",
    "The network behave as a function `N`, generating a prediction `P` of what it is.\n",
    "\n",
    "We can write`N(S) = P`\n",
    "\n",
    "We know what the answer `A` should be (this is the label we extracted, in the `Y` variable !). \n",
    "To know how much the network is mistaken, we will calculate the error `L` that we call the loss.\n",
    "\n",
    "There are a lot of different loss. I chose to use a binary_crossentropy but to give you a simpler one for description purpose, imagine that we have `L = P - A`.\n",
    "\n",
    "The purpose of the neural network is to reduce `L` to its minimum. This system will lead to reducing the difference between our prediction `P` and the answer `A`.\n",
    "\n",
    "However, if we were to show <b>all</b> samples, the network may just find statistical anomalies to make its decision. In somehow, you could compare it to a student learning by heart the answer to all questions of the test. It can be good for some subject like history... but for math, imaigne now a student who learned this way, without understanding the mathematical object themselves. As soon as you cange the values of the mathematical equations, the student will not be able to solve the problems given.\n",
    "\n",
    "A similar problem may appear here, where our network learns all the question by heart. This is what we call <b>overfitting</b>. And overfitting is bad. To check if we are learning from the dataset or simply memorizing it (and thus, no use of the neural network...), we take apart of our samples (20%), that the model will not train on.\n",
    "\n",
    "If our network can solve the 20% it has never seen, it means it will have learned to solve the problem. Of course, this should also implement a balanced representation of the different output. If the model only returns \"this is a normal connection\" and you only test on normal connection, you will see good results... but you will not find your model is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79496     0]\n",
      " [    0 19309]]\n"
     ]
    }
   ],
   "source": [
    "#We test for balance\n",
    "print(confusion_matrix(y_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we do not have the same quantity between \"normal\" and \"abnormal\". However, we have enoug samples to control if the network learnt or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "We are now ready to train the network. We could use the testing set we have made to check the model does not overfit while we train it... but I decided to choose to use a validation dataset. This is 20% of the 80% we have in the Training Dataset. This will be displayed as we train.\n",
    "\n",
    "Next, we will be able to see how we perform on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 316172 samples, validate on 79044 samples\n",
      "Epoch 1/10\n",
      "316172/316172 [==============================] - 8s 26us/sample - loss: 0.0666 - acc: 0.9880 - val_loss: 0.0265 - val_acc: 0.9931\n",
      "Epoch 2/10\n",
      "316172/316172 [==============================] - 8s 25us/sample - loss: 0.0256 - acc: 0.9929 - val_loss: 0.0246 - val_acc: 0.9936\n",
      "Epoch 3/10\n",
      "316172/316172 [==============================] - 8s 25us/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.0166 - val_acc: 0.9949\n",
      "Epoch 4/10\n",
      "316172/316172 [==============================] - 8s 27us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0140 - val_acc: 0.9951\n",
      "Epoch 5/10\n",
      "316172/316172 [==============================] - 9s 27us/sample - loss: 0.0151 - acc: 0.9950 - val_loss: 0.0119 - val_acc: 0.9954\n",
      "Epoch 6/10\n",
      "316172/316172 [==============================] - 8s 26us/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.0097 - val_acc: 0.9956\n",
      "Epoch 7/10\n",
      "316172/316172 [==============================] - 8s 26us/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0075 - val_acc: 0.9963\n",
      "Epoch 8/10\n",
      "316172/316172 [==============================] - 8s 26us/sample - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0061 - val_acc: 0.9989\n",
      "Epoch 9/10\n",
      "316172/316172 [==============================] - 8s 27us/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0059 - val_acc: 0.9988\n",
      "Epoch 10/10\n",
      "316172/316172 [==============================] - 9s 27us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0059 - val_acc: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fee3d9a278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          batch_size=128, #We show 128 samples to compute the loss.\n",
    "          epochs=10, #And will see 10 times the dataset.\n",
    "          validation_split = 0.2) #The 20% we take from the Training for the validation training set\n",
    "#(The network is not trained on the validation. It is only on the testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is trained and Keras tells us that we have more than 99.88% of accuracy on the validation test (that he never saw!).\n",
    "\n",
    "Let's test it with our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix ANN:\n",
      "[[79418    78]\n",
      " [   37 19272]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test);\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "print(\"confusion matrix ANN:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a small number of false positive and false negative. It seems our model has indeed correctly classify the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
